{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Assignment\n",
    "\n",
    "In this assignment, you will implement a Support Vector Machine Classifier  from scratch and compare the results to existing sklearn algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# make this notebook's output stable across runs\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChenWei I added this class to see a working example of SVM\n",
    "# I was having trouble with my own algorithm and this helped me at least a function that worked.\n",
    "# This comes from AssemblyAI series on ML from scratch\n",
    "# https://www.youtube.com/playlist?list=PLcWfeUsAys2k_xub3mHks85sBHZvg24Jd\n",
    "# used this reference code to help understand and debug my implementation\n",
    "class SupportVectorMachine:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None   #weights\n",
    "        self.b = 0      #biases\n",
    "\n",
    "    def fit(self, X: np.ndarray, y:np.ndarray, random_weights=False) -> None:\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        if random_weights:\n",
    "            self.w = np.random.uniform(-1.0, 1.0, n_features)\n",
    "        else:\n",
    "            self.w = np.zeros(n_features)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.1: Implement the cost function cost/objective function:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/688/1*JAS6rUTO7TDlrv4XZSMbsA.png\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the cost/objective function for Support Vector Machine\n",
    "# reg_strength is C in the above equation\n",
    "def compute_cost(W, X, Y, regularization_strength=1000):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0] # number of training examples\n",
    "    distances = 1 - Y * (np.dot(W, X.T).flatten())\n",
    "    distances[distances < 0] = 0  # convert anything less than 0 to 0\n",
    "\n",
    "    hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
    "\n",
    "    # calculate cost\n",
    "    cost = 0.5 * np.dot(W, W)**2 + hinge_loss\n",
    "    return cost\n",
    "\n",
    "def compute_cost2(W, X, Y, regularization_strength=1000):\n",
    "    w_norm = 0.5*np.dot(W, W)**2\n",
    "    offsets = Y * (np.dot(W, X.T).flatten())\n",
    "    smax = np.clip(a=1-offsets, a_min=0.0, a_max=np.inf)\n",
    "    hinge_loss = regularization_strength * np.sum(smax)/len(smax)\n",
    "    cost = w_norm + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.2: Write a method that calculate the cost gradient:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/866/1*ww3F21VMVGp2NKhm0VTesA.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch, regularization_strength=1000):\n",
    "\n",
    "   dw = np.zeros(len(W))\n",
    "   N = len(Y_batch)\n",
    "   # compute the losses up front\n",
    "   # doing this outside the loop speeds up the gradient calculation\n",
    "   losses = 1 - Y_batch * (np.dot(X_batch, W))\n",
    "   losses[losses < 0] = 0  # convert anything less than 0 to 0\n",
    "   for x, y, hinge_loss in zip(X_batch, Y_batch, losses):\n",
    "       if hinge_loss == 0:\n",
    "           dw = W\n",
    "       else:\n",
    "           dw = W - (regularization_strength * y * x)\n",
    "\n",
    "   dw = dw/N  # average\n",
    "   return dw\n",
    "\n",
    "def calculate_cost_gradient2(W, X_batch, Y_batch, regularization_strength=1000):\n",
    "   dw = np.zeros(W.shape[0])\n",
    "   for x, y in zip(X_batch, Y_batch):\n",
    "       hinge_loss = np.max((0, 1.0 - np.dot(W.T, x)*y))\n",
    "       if hinge_loss == 0:\n",
    "           dw += W\n",
    "       else:\n",
    "           dw += W - (regularization_strength * y * x)\n",
    "   return dw/len(Y_batch)  # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.3: Write a method that performs stochastic Gradient descent as follows:\n",
    "- Calculate the gradient of cost function i.e. ∇J(w)\n",
    "- Update the weights in the opposite direction to the gradient: w = w — ∝(∇J(w))\n",
    "- Repeat until conversion or until 5000 epochs are reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(data, outputs, learning_rate = 0.0001, max_epochs = 4096, cost_threshold=0.01, random_weights=False):\n",
    "    print(\"Training using Stochastic Gradient Descent 1 method\")\n",
    "    if random_weights:\n",
    "       weights = np.random.uniform(-1.0, 1.0, data.shape[1])\n",
    "    else:\n",
    "       weights = np.zeros(data.shape[1])\n",
    "\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(data, outputs)\n",
    "        for x_chunk, y_chunk in zip(np.array_split(X, 100), np.array_split(Y, 100)):\n",
    "            ascent = calculate_cost_gradient(weights, x_chunk, y_chunk)\n",
    "            weights = weights - learning_rate * ascent\n",
    "            # convergence check on 2^nth epoch\n",
    "            if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "                cost = compute_cost(weights, data, outputs)\n",
    "                print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "                # stoppage criterion\n",
    "                if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                    return weights\n",
    "                prev_cost = cost\n",
    "                nth += 1\n",
    "               \n",
    "    return weights\n",
    "\n",
    "def sgd2(data, outputs, learning_rate = 0.0001, max_epochs = 4096, cost_threshold = 0.01):\n",
    "   print(\"Training using Stochastic Gradient Descent 2 method\")\n",
    "   weights = np.random.uniform(-1.0, 1.0, data.shape[1])\n",
    "   nth = 1\n",
    "   prev_cost = 0.0\n",
    "   # stochastic gradient descent\n",
    "   for epoch in range(1, max_epochs):\n",
    "      X, Y = shuffle(data, outputs)\n",
    "      for x_chunk, y_chunk in zip(np.array_split(X, 100), np.array_split(Y, 100)):\n",
    "         ascent = calculate_cost_gradient2(weights, x_chunk, y_chunk)\n",
    "         weights = weights - learning_rate * ascent\n",
    "\n",
    "      if epoch == 2 ** nth:\n",
    "         cost = compute_cost2(weights, X, Y)\n",
    "         print(f\"Epoch is:{epoch} and Cost is: {cost}\")\n",
    "         # stoppage criterion\n",
    "         if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "            return weights\n",
    "         prev_cost = cost\n",
    "         nth += 1\n",
    "   \n",
    "   return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_banknote_authentication.csv')\n",
    "\n",
    "Y = data.iloc[:, -1]*2-1\n",
    "X = data.iloc[:, 1:4]\n",
    "X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Train and evaluate an SVC using the banknote_authentication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with sgd()\n",
      "Training using Stochastic Gradient Descent 1 method\n",
      "Epoch is:1 and Cost is: 1365.1217913107123\n",
      "Epoch is:2 and Cost is: 703.6417932095144\n",
      "Epoch is:4 and Cost is: 483.6408403782307\n",
      "Epoch is:8 and Cost is: 490.1201811982653\n",
      "Epoch is:16 and Cost is: 524.5413041533213\n",
      "Epoch is:32 and Cost is: 441.5748957071105\n",
      "Epoch is:64 and Cost is: 593.1599946387327\n",
      "Epoch is:128 and Cost is: 470.9467724498545\n",
      "Epoch is:256 and Cost is: 475.2087574090331\n",
      "Training time for sgd(): 0.291914701461792 seconds ---\n",
      "training finished.\n",
      "weights are: [-0.61613457 -0.39892281 -0.45951297  0.88392396]\n",
      "accuracy on test dataset: 0.8214936247723132\n",
      "\n",
      "training with sgd2()\n",
      "Training using Stochastic Gradient Descent 2 method\n",
      "Epoch is:2 and Cost is: 509.4665350995559\n",
      "Epoch is:4 and Cost is: 656.3647361867406\n",
      "Epoch is:8 and Cost is: 622.8166242966822\n",
      "Epoch is:16 and Cost is: 548.8379896797907\n",
      "Epoch is:32 and Cost is: 536.4863906211289\n",
      "Epoch is:64 and Cost is: 505.32978523719675\n",
      "Epoch is:128 and Cost is: 525.1111817116611\n",
      "Epoch is:256 and Cost is: 506.4309716986987\n",
      "Epoch is:512 and Cost is: 534.2077282008946\n",
      "Epoch is:1024 and Cost is: 481.3557459298218\n",
      "Epoch is:2048 and Cost is: 698.0834469219999\n",
      "Training time for sgd2(): 14.38266634941101 seconds ---\n",
      "training finished.\n",
      "weights are: [-0.65116504 -0.46465195 -0.66720066  0.95618011]\n",
      "accuracy on test dataset: 0.8105646630236795\n"
     ]
    }
   ],
   "source": [
    "# train the model using SGD1\n",
    "# compute the time to calculate sgd\n",
    "\n",
    "print(\"training with sgd()\")\n",
    "start_time = time.time()\n",
    "W1 = sgd(X_train.to_numpy(), y_train.to_numpy(), cost_threshold=0.01, random_weights=True)\n",
    "print(\"Training time for sgd(): %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"training finished.\")\n",
    "print(\"weights are: {}\".format(W1))\n",
    "\n",
    "\n",
    "# testing the model on test set\n",
    "sgd_predicted = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    yp = np.sign(np.dot(X_test.to_numpy()[i], W1))\n",
    "    sgd_predicted = np.append(sgd_predicted, yp)\n",
    "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), sgd_predicted)))\n",
    "\n",
    "\n",
    "#train the model using SGD2\n",
    "print(\"\\ntraining with sgd2()\")\n",
    "start_time = time.time()\n",
    "W2 = sgd2(X_train.to_numpy(), y_train.to_numpy(), cost_threshold=0.01)\n",
    "print(\"Training time for sgd2(): %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"training finished.\")\n",
    "print(\"weights are: {}\".format(W2))\n",
    "\n",
    "\n",
    "# testing the model on test set\n",
    "sgd2_predicted = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    yp = np.sign(np.dot(X_test.to_numpy()[i], W2))\n",
    "    sgd2_predicted = np.append(sgd2_predicted, yp)\n",
    "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), sgd2_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for SupportVectorMachine(): 2.2477619647979736 seconds ---\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.8214936247723132\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    '''\n",
    "    Define a function to test accuracy of the model\n",
    "    '''\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# This is using the reference code from AssemblyAI series on ML from scratch\n",
    "clf = SupportVectorMachine()\n",
    "start_time = time.time()\n",
    "clf.fit(X_train.to_numpy(), y_train.to_numpy(), random_weights=True)\n",
    "print(\"Training time for SupportVectorMachine(): %s seconds ---\" % (time.time() - start_time))\n",
    "predictions = clf.predict(X_test)\n",
    "print (type(predictions))\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bonus] Question 5: Train and evaluate an SKLEARN SVC model, and compare the results to your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test dataset using sklearn: 0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "# Train a Support Vector Machine model from Sklearn using the same data as inputs\n",
    "from sklearn.svm import SVC\n",
    "#clf = SVC(kernel='linear')\n",
    "clf = SVC(kernel='rbf') # I tried all different kernels and rbf gave the best accuracy\n",
    "clf.fit(X_train, y_train)\n",
    "sgd_predicted = clf.predict(X_test)\n",
    "print(\"accuracy on test dataset using sklearn: {}\".format(accuracy_score(y_test.to_numpy(), sgd_predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Create a new text cell in your Notebook: Complete a 50-100 word summary (or short description of your thinking in applying this week's learning to the solution) of your experience in this assignment. Include: What was your incoming experience with this model, if any? what steps you took, what obstacles you encountered. how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?) This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel like I learned the basics of SVM pretty well by implementing the SVM algorithm from scratch. My understanding of it (I have studied it a little bit before now as well.) is that it makes an attempt to find a hyperplane that divides the two classes of data points the best. It does this through a stochastic gradient descent algorithm. I still feel like I'm not entirely sure why exactly the loss function and the cost functions are best for this algorithm. Maybe I'll take that a little bit on faith because it appears to work well.\n",
    "\n",
    "This week i actually have 3 slightly different implementations of stochastic gradient descent as part of the SVM algorithm. Really the heart of SVM is SGD.\n",
    "\n",
    "1. The first implementation is loosely based on the resource provided by Chenwei in class. However that implementation didn't work. So I modified it until it worked.\n",
    "2. The SVM class at the top of the file is the second implementation. It is probably the most straightforward implementation of SGD here.\n",
    "3. I also included the SGD2() function, which is basically CHenwei's implementation.\n",
    "4. Finally as the assignment directed, I trained the SVM model from sklearn. Unsurprisingly this performed best, but I had to use trial and error to find the best kernel function to achieve highest accuracy.\n",
    "\n",
    "I wanted to compare the 3 algorithms that I implemented. As can be seen they all performed roughly the same on this weeks Bank NOte data set.\n",
    "\n",
    "1. SGD() had accuracy = 82.14%\n",
    "2. SGD2() had accuracy = 81.05%\n",
    "3. the SVM class from Assembly AI had accuracy = 82.14% as well\n",
    "4. Sklearn, being a professionally optimized library written in C++, of course had the best accuracy = 90.16%\n",
    "\n",
    "Another interesting thing is the timing info gathered between SGD() and SGD2()\n",
    "\n",
    "The time to train those two different models is quite different. SGD() is orders of magnitude faster than SGD2(). This comes down to one thing. I computed the hinge loss for an entire chunk up front using numpy vectorization. This is much faster than computing the hinge loss for each data point individually.\n",
    "\n",
    "This optimization can be seen in the function ```calculate_cost_gradient```. In particular I calculate:\n",
    "\n",
    "```\n",
    "   # compute the losses up front\n",
    "   # doing this outside the loop speeds up the gradient calculation\n",
    "   losses = 1 - Y_batch * (np.dot(X_batch, W))\n",
    "   losses[losses < 0] = 0  # convert anything less than 0 to 0\n",
    "```\n",
    "Then later use these losses when computing the gradient. This one change had a large positive performance impact. As can be seen in the timing info below.\n",
    "\n",
    "```\n",
    "   Training time for sgd(): 0.302901029586792 seconds\n",
    "   Training time for sgd2(): 14.218122243881226 seconds\n",
    "```\n",
    "\n",
    "Another smaller optimization that had a positive impact on performance is initializing the Weights randomly instead of with all zeros. I added a parameter to both sgd() and sgd2() to use this optimization. In practice, I think it would always be better to have random weights instead of zeroed weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
