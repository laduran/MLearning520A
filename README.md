## Lesson 01 Overview

**Introduction to Advanced Machine Learning and Real-World Application**

### Learning Topics 
 - Supervised learning
   - Regression
   - Binary classification
   - Multi-class classification
 - Review
   - Linear model vs. Non-linear model
   - Regularization
   - Hyper-parameters
   - Model evaluation
 - Case Study

### Learning Objectives

You will be able to:

 - Implement two strategies for multiclass classification (lab).
 - Apply implementation to data set .
 - Apply insights gained from exploring a real-world case to thinking about how to address real-world problems, starting with a machine learning problem, then applying techniques.

### Before Class 

Review ISLR readings from MLearn 510 in the areas of topics to be reviewed this lesson and assemble in-class questions.

## Lesson 02 Overview
**Decision Trees**

### Learning Topics 
 - Gini impurity
 - Entropy
 - Information gain
 - Classification tree
 - Regression tree
 - Variable importance

### Learning Objectives
You will be able to:

 - Explain/demonstrate how Gini impurity is used for constructing decision tree.
 - Compare entropy to Gini impurity.
 - Explain/demonstrate how information gain is used for constructing decision tree.
 - Differentiate between classification tree vs. regression tree.
 - Apply recursive partitioning used for constructing classification / regression tree.
 - Tune
 - Apply regularization to decision tree to prevent overfitting.
 - Extract variable importance to interpret the decision tree model.

### Before Class 

Please complete the following to prepare for this week's class:

Read ISLR Section 8.1 and assemble in-class questions.
Review [ISLR slides:](https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/trees.pdfLinks) to an external site.
Review lecture videos - Assemble your in-class questions

## Lesson 03 Overview
**Random Forest and Gradient Boosted Trees**

### Learning Topics
 - REVIEW
   - Bias and Variance tradeoff
   - Bootstrap
 - Decision/Regression tree bagging
 - Random forest regression/classification
 - Gradient boosting regression/classification

### Learning Objectives

You will be able to:

 - Describe bootstrap and why bootstrap helps.
 - Demonstrate how bootstrap aggregation works for classification/regression trees.
 - Apply random forest and gradient boosting trees to data sets and evaluate performance.

### Before Class 
Please complete the following to prepare for this week's class:

 - Read ISLR 8.2 to the end of 8 and assemble in-class questions.
 - Review lecture videos - Assemble your in-class questions

## Lesson 04 Overview
**Support Vector Machine (SVM)**

### Learning Topics

### Learning Objectives

### Before Class