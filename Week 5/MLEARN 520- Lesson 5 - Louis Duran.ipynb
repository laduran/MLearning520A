{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Assignment\n",
    "\n",
    "In this assignment, you will implement a Support Vector Machine Classifier  from scratch and compare the results to existing sklearn algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# make this notebook's output stable across runs\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from AssemblyAI series on ML from scratch\n",
    "# https://www.youtube.com/playlist?list=PLcWfeUsAys2k_xub3mHks85sBHZvg24Jd\n",
    "# used this reference code to help understand and debug my implementation\n",
    "class SupportVectorMachine:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None   #weights\n",
    "        self.b = None   #biases\n",
    "\n",
    "    def fit(self, X: np.ndarray, y:np.ndarray):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        # init the weights\n",
    "        self.w = np.zeros(n_features)  # Again it would better to randomly initialize the weights\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.1: Implement the cost function cost/objective function:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/688/1*JAS6rUTO7TDlrv4XZSMbsA.png\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the cost/objective function for Support Vector Machine\n",
    "# reg_strength is C in the above equation\n",
    "def compute_cost(W, X, Y, regularization_strength=1000):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0] # number of training examples\n",
    "    distances = 1 - Y * (np.dot(W, X.T).flatten())\n",
    "    distances[distances < 0] = 0  # convert anything less than 0 to 0\n",
    "\n",
    "    hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
    "\n",
    "    # calculate cost\n",
    "    cost = 0.5 * np.dot(W, W)**2 + hinge_loss\n",
    "    return cost\n",
    "\n",
    "def compute_cost2(W, X, Y, regularization_strength=1000):\n",
    "    w_norm = 0.5*np.dot(W, W)**2\n",
    "    offsets = Y * (np.dot(W, X.T).flatten())\n",
    "    smax = np.clip(a=1 - offsets, a_min=0.0, a_max=np.inf)\n",
    "    hinge_loss = regularization_strength * np.sum(smax)/len(smax)\n",
    "    cost = w_norm + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.2: Write a method that calculate the cost gradient:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/866/1*ww3F21VMVGp2NKhm0VTesA.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch, regularization_strength=1000):\n",
    "\n",
    "   dw = np.zeros(len(W))\n",
    "   N = len(Y_batch)\n",
    "   for x, y in zip(X_batch, Y_batch):\n",
    "       hinge_loss = np.max((0, 1.0 - np.dot(W.T, x)))\n",
    "       if hinge_loss == 0:\n",
    "           dw = W\n",
    "       else:\n",
    "           dw = W - (regularization_strength * y * x)\n",
    "\n",
    "   dw = dw/N  # average\n",
    "   return dw\n",
    "\n",
    "def calculate_cost_gradient2(W, X_batch, Y_batch, regularization_strength=1000):\n",
    "   dw = np.zeros(W.shape[0])\n",
    "   for x, y in zip(X_batch, Y_batch):\n",
    "       hinge_loss = np.max((0, 1.0 - np.dot(W.T, x)*y))\n",
    "       if hinge_loss == 0:\n",
    "           dw += W\n",
    "       else:\n",
    "           dw += W - (regularization_strength * y * x)\n",
    "   return dw/len(Y_batch)  # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.3: Write a method that performs stochastic Gradient descent as follows:\n",
    "- Calculate the gradient of cost function i.e. ∇J(w)\n",
    "- Update the weights in the opposite direction to the gradient: w = w — ∝(∇J(w))\n",
    "- Repeat until conversion or until 5000 epochs are reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(data, outputs, learning_rate = 0.0001, max_epochs = 5000):\n",
    "    print(\"Training using Stochastic Gradient Descent 2 method\")\n",
    "    weights = np.zeros(data.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01  # in percent\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(data, outputs)\n",
    "        #for ind, x in enumerate(X):\n",
    "        for x_chunk, y_chunk in zip(np.array_split(X, 100), np.array_split(Y, 100)):\n",
    "            ascent = calculate_cost_gradient(weights, x_chunk, y_chunk)\n",
    "            weights = weights - learning_rate * ascent\n",
    "            # convergence check on 2^nth epoch\n",
    "            if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "                cost = compute_cost(weights, data, outputs)\n",
    "                print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "                # stoppage criterion\n",
    "                if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                    return weights\n",
    "                prev_cost = cost\n",
    "                nth += 1\n",
    "               \n",
    "    return weights\n",
    "\n",
    "def sgd2(data, outputs, learning_rate = 0.0001, max_epochs = 5000):\n",
    "   print(\"Training using Stochastic Gradient Descent 2 method\")\n",
    "   weights = np.random.uniform(-1.0, 1.0, data.shape[1])\n",
    "   nth = 1\n",
    "   prev_cost = 0.0\n",
    "   cost_threshold = 0.01  # in percent\n",
    "   # stochastic gradient descent\n",
    "   for epoch in range(1, max_epochs):\n",
    "      X, Y = shuffle(data, outputs)\n",
    "      for x_chunk, y_chunk in zip(np.array_split(X, 100), np.array_split(Y, 100)):\n",
    "         ascent = calculate_cost_gradient(weights, x_chunk, y_chunk)\n",
    "         weights = weights - learning_rate * ascent\n",
    "\n",
    "      if epoch == 2 ** nth:\n",
    "         cost = compute_cost(weights, X, Y)\n",
    "         print(f\"Epoch is:{epoch} and Cost is: {cost}\")\n",
    "         # stoppage criterion\n",
    "         if epoch == max_epochs - 1:\n",
    "            return weights\n",
    "         prev_cost = cost\n",
    "         nth += 1\n",
    "   \n",
    "   return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_banknote_authentication.csv')\n",
    "\n",
    "Y = data.iloc[:, -1]*2-1\n",
    "X = data.iloc[:, 1:4]\n",
    "X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Train and evaluate an SVC using the banknote_authentication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "Training using Stochastic Gradient Descent 2 method\n",
      "Epoch is:1 and Cost is: 760.9746869905896\n",
      "Epoch is:2 and Cost is: 3002.8950286788067\n",
      "Epoch is:4 and Cost is: 9351.74411788382\n",
      "Epoch is:8 and Cost is: 63518.81816118541\n",
      "Epoch is:16 and Cost is: 929557.8688096321\n",
      "Epoch is:32 and Cost is: 19636087.966861937\n",
      "Epoch is:64 and Cost is: 286094655.6427475\n",
      "Epoch is:128 and Cost is: 3770759755.5630956\n",
      "Epoch is:256 and Cost is: 49743003231.08576\n",
      "Epoch is:512 and Cost is: 456000677754.7444\n",
      "Epoch is:1024 and Cost is: 2649797901203.2236\n",
      "Epoch is:2048 and Cost is: 6975087033201.291\n",
      "Epoch is:4096 and Cost is: 9810148505873.498\n",
      "Epoch is:4999 and Cost is: 9883564727670.102\n",
      "training finished.\n",
      "weights are: [-2070.40506174  -315.85191267   109.86784152  -218.20410039]\n",
      "accuracy on test dataset: 0.6502732240437158\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"training started...\")\n",
    "W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
    "print(\"training finished.\")\n",
    "print(\"weights are: {}\".format(W))\n",
    "\n",
    "# testing the model on test set\n",
    "y_test_predicted = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
    "    y_test_predicted = np.append(y_test_predicted, yp)\n",
    "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.8214936247723132\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    '''\n",
    "    Define a function to test accuracy of the model\n",
    "    '''\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# This is using the reference code from AssemblyAI series on ML from scratch\n",
    "clf = SupportVectorMachine()\n",
    "clf.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "predictions = clf.predict(X_test)\n",
    "print (type(predictions))\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bonus] Question 5: Train and evaluate an SKLEARN SVC model, and compare the results to your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test dataset using sklearn: 0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "# Train a Support Vector Machine model from Sklearn using the same data as inputs\n",
    "from sklearn.svm import SVC\n",
    "#clf = SVC(kernel='linear')\n",
    "clf = SVC(kernel='rbf') # I tried all different kernels and rbf gave the best accuracy\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_predicted = clf.predict(X_test)\n",
    "print(\"accuracy on test dataset using sklearn: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Create a new text cell in your Notebook: Complete a 50-100 word summary (or short description of your thinking in applying this week's learning to the solution) of your experience in this assignment. Include: What was your incoming experience with this model, if any? what steps you took, what obstacles you encountered. how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?) This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
